#!/usr/bin/env python3
"""
å°† HuggingFace BERT æƒé‡è½¬æ¢ä¸º BERT-pytorch æ ¼å¼ï¼ˆå®Œå…¨æ­£ç¡®ç‰ˆæœ¬ï¼‰
åŸºäº BERT-pytorch å®é™…æºä»£ç çš„å‡†ç¡®æ˜ å°„
"""

import torch
import sys
import os
import argparse

def detect_model_config(hf_state_dict):
    """
    ä» HuggingFace state_dict è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®

    Returns:
        dict: åŒ…å« vocab_size, hidden, n_layers, attn_heads
    """
    # æ£€æµ‹ vocab_size
    vocab_size = hf_state_dict['bert.embeddings.word_embeddings.weight'].shape[0]

    # æ£€æµ‹ hidden size
    hidden = hf_state_dict['bert.embeddings.word_embeddings.weight'].shape[1]

    # æ£€æµ‹å±‚æ•°
    n_layers = 0
    for key in hf_state_dict.keys():
        if 'bert.encoder.layer.' in key:
            layer_num = int(key.split('.')[3])
            n_layers = max(n_layers, layer_num + 1)

    # æ£€æµ‹æ³¨æ„åŠ›å¤´æ•° (é€šè¿‡ query æƒé‡å½¢çŠ¶æ¨æ–­)
    # query weight shape: [hidden, hidden]
    # attn_heads = hidden / head_dim, head_dim = hidden / attn_heads
    # å¯¹äº BERTï¼Œé€šå¸¸ head_dim = 64
    attn_heads = hidden // 64

    model_type = "BERT-Large" if n_layers == 24 else "BERT-Base"

    config = {
        'vocab_size': vocab_size,
        'hidden': hidden,
        'n_layers': n_layers,
        'attn_heads': attn_heads,
        'model_type': model_type
    }

    return config

def convert_huggingface_to_bert_pytorch(hf_state_dict, model_config=None):
    """
    å°† HuggingFace æ ¼å¼çš„æƒé‡è½¬æ¢ä¸º BERT-pytorch æ ¼å¼

    å…³é”®å·®å¼‚ï¼ˆå…¨éƒ¨å·²ä¿®å¤ï¼‰ï¼š
    1. Position Embedding: å·²ä¿®å¤ä¸ºå­¦ä¹ çš„ nn.Embedding âœ“
    2. Embedding LayerNorm: å·²æ·»åŠ  âœ“
    3. LayerNorm å‚æ•°: a_2/b_2 è€Œä¸æ˜¯ weight/bias
    4. Attention è·¯å¾„: attention.linear_layers è€Œä¸æ˜¯ attention.attention.linear_layers
    5. Sublayer: input_sublayer å’Œ output_sublayer

    Args:
        hf_state_dict: HuggingFace æ¨¡å‹çš„ state_dict
        model_config: æ¨¡å‹é…ç½®å­—å…¸ (å¯é€‰ï¼Œå°†è‡ªåŠ¨æ£€æµ‹)
    """

    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®
    if model_config is None:
        model_config = detect_model_config(hf_state_dict)
        print(f"\nğŸ“Š è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®:")
        print(f"  æ¨¡å‹ç±»å‹:     {model_config['model_type']}")
        print(f"  è¯æ±‡è¡¨å¤§å°:   {model_config['vocab_size']}")
        print(f"  éšè—å±‚ç»´åº¦:   {model_config['hidden']}")
        print(f"  Transformerå±‚: {model_config['n_layers']}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°:   {model_config['attn_heads']}")

    converted = {}
    converted_count = 0
    skipped_count = 0

    print("\nå¼€å§‹è½¬æ¢æƒé‡...")
    print("=" * 80)

    for key, value in hf_state_dict.items():
        new_key = None

        # ===== Embeddings =====

        # Token Embedding
        if key == 'bert.embeddings.word_embeddings.weight':
            new_key = 'embedding.token.weight'

        # Position Embedding - ç°åœ¨è½¬æ¢ï¼ˆå·²ä¿®å¤ä¸ºå­¦ä¹ çš„ Embeddingï¼‰
        elif key == 'bert.embeddings.position_embeddings.weight':
            new_key = 'embedding.position.pe.weight'
            print(f"âœ“ {key:70s} -> {new_key}")
            print(f"   ğŸ“ ä½ç½®ç¼–ç ç°åœ¨æ˜¯å¯å­¦ä¹ çš„ nn.Embeddingï¼ˆå·²ä¿®å¤ï¼‰")

        # Segment Embedding
        elif key == 'bert.embeddings.token_type_embeddings.weight':
            new_key = 'embedding.segment.weight'
            # HuggingFace: [2, 768] -> BERT-pytorch: [3, 768]
            if value.shape[0] == 2:
                print(f"ğŸ“ æ‰©å±• segment embedding: {value.shape} -> [3, {value.shape[1]}]")
                new_value = torch.zeros(3, value.shape[1])
                new_value[1] = value[0]  # å¥å­ A
                new_value[2] = value[1]  # å¥å­ B
                value = new_value

        # Embedding LayerNorm - ç°åœ¨è½¬æ¢ï¼ˆå·²æ·»åŠ åˆ° BERTEmbeddingï¼‰
        elif key in ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.gamma']:
            new_key = 'embedding.layer_norm.weight'
        elif key in ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.beta']:
            new_key = 'embedding.layer_norm.bias'

        # ===== Encoder Layers =====

        elif 'bert.encoder.layer.' in key:
            parts = key.split('.')
            layer_num = parts[3]  # bert.encoder.layer.{i}...

            # --- Attention: Query, Key, Value ---
            if 'attention.self.query.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.0.weight'
            elif 'attention.self.query.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.0.bias'

            elif 'attention.self.key.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.1.weight'
            elif 'attention.self.key.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.1.bias'

            elif 'attention.self.value.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.2.weight'
            elif 'attention.self.value.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.linear_layers.2.bias'

            # --- Attention Output Dense ---
            elif 'attention.output.dense.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.output_linear.weight'
            elif 'attention.output.dense.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.attention.output_linear.bias'

            # --- Attention Output LayerNorm (input_sublayer) ---
            elif 'attention.output.LayerNorm.weight' in key or 'attention.output.LayerNorm.gamma' in key:
                new_key = f'transformer_blocks.{layer_num}.input_sublayer.norm.a_2'
            elif 'attention.output.LayerNorm.bias' in key or 'attention.output.LayerNorm.beta' in key:
                new_key = f'transformer_blocks.{layer_num}.input_sublayer.norm.b_2'

            # --- Feed Forward: Intermediate ---
            elif 'intermediate.dense.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.feed_forward.w_1.weight'
            elif 'intermediate.dense.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.feed_forward.w_1.bias'

            # --- Feed Forward: Output Dense ---
            elif 'output.dense.weight' in key:
                new_key = f'transformer_blocks.{layer_num}.feed_forward.w_2.weight'
            elif 'output.dense.bias' in key:
                new_key = f'transformer_blocks.{layer_num}.feed_forward.w_2.bias'

            # --- Feed Forward Output LayerNorm (output_sublayer) ---
            elif 'output.LayerNorm.weight' in key or 'output.LayerNorm.gamma' in key:
                new_key = f'transformer_blocks.{layer_num}.output_sublayer.norm.a_2'
            elif 'output.LayerNorm.bias' in key or 'output.LayerNorm.beta' in key:
                new_key = f'transformer_blocks.{layer_num}.output_sublayer.norm.b_2'

        # ===== Pooler & Classification Heads (è·³è¿‡) =====

        elif 'bert.pooler' in key:
            skipped_count += 1
            continue

        elif key.startswith('cls.'):
            skipped_count += 1
            continue

        else:
            print(f"âš ï¸  æœªçŸ¥çš„ key: {key}")
            skipped_count += 1
            continue

        # ä¿å­˜è½¬æ¢åçš„æƒé‡
        if new_key:
            converted[new_key] = value
            converted_count += 1

            # åªæ‰“å°å‰ 10 ä¸ªå’Œæœ€å 10 ä¸ª
            if converted_count <= 10 or converted_count > 180:
                print(f"âœ“ {key:70s} -> {new_key}")
            elif converted_count == 11:
                print(f"... (çœç•¥ä¸­é—´çš„è½¬æ¢ä¿¡æ¯)")

    print("=" * 80)
    print(f"è½¬æ¢å®Œæˆ:")
    print(f"  âœ… æˆåŠŸè½¬æ¢: {converted_count} ä¸ªå‚æ•°")
    print(f"  â­ï¸  è·³è¿‡: {skipped_count} ä¸ªå‚æ•°")
    print(f"  ğŸ“Š æ€»è®¡: {len(hf_state_dict)} ä¸ªå‚æ•°")

    # æ ¹æ®æ¨¡å‹ç±»å‹è®¡ç®—æœŸæœ›å‚æ•°æ•°é‡
    # BERT-Base: 195, BERT-Large: 387
    expected_params = 195 if model_config['n_layers'] == 12 else 387
    print(f"\næœŸæœ›æ¨¡å‹èƒ½åŠ è½½: {converted_count}/{expected_params} ä¸ªå‚æ•°")

    return converted, model_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='HuggingFace BERT æƒé‡è½¬æ¢å·¥å…· (æ”¯æŒ BERT-Base å’Œ BERT-Large)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹é…ç½®
  python3 convert_hf_to_bert_pytorch.py bert-base-uncased.bin
  python3 convert_hf_to_bert_pytorch.py bert-large-uncased.bin

  # æ‰‹åŠ¨æŒ‡å®šé…ç½®
  python3 convert_hf_to_bert_pytorch.py bert-large.bin bert-large-converted.bin --hidden 1024 --layers 24 --heads 16
        """
    )

    parser.add_argument('input', help='è¾“å…¥æƒé‡æ–‡ä»¶ (.bin)')
    parser.add_argument('output', nargs='?', help='è¾“å‡ºæƒé‡æ–‡ä»¶ (.binï¼Œå¯é€‰)')
    parser.add_argument('--vocab_size', type=int, help='è¯æ±‡è¡¨å¤§å° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--hidden', type=int, help='éšè—å±‚ç»´åº¦ (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--layers', type=int, help='Transformerå±‚æ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--heads', type=int, help='æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')

    args = parser.parse_args()

    # è®¾ç½®é»˜è®¤è¾“å‡ºè·¯å¾„
    if args.output is None:
        base_name = os.path.splitext(args.input)[0]
        args.output = f"{base_name}-converted.bin"

    return args

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("HuggingFace BERT æƒé‡è½¬æ¢å·¥å…· (æ”¯æŒ BERT-Base å’Œ BERT-Large)")
    print("åŸºäº BERT-pytorch å®é™…æºä»£ç ç»“æ„")
    print("=" * 80)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    input_path = args.input
    output_path = args.output

    print(f"\nè¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_path):
        print(f"\nâŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return 1

    # åŠ è½½ HuggingFace æƒé‡
    print(f"\nğŸ“¥ åŠ è½½ HuggingFace æƒé‡...")
    try:
        hf_state_dict = torch.load(input_path, map_location='cpu', weights_only=False)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(hf_state_dict)} ä¸ªå‚æ•°")

        # æ£€æµ‹å‘½åæ ¼å¼
        has_gamma_beta = any('gamma' in k or 'beta' in k for k in hf_state_dict.keys())
        if has_gamma_beta:
            print(f"   ğŸ“Œ æ£€æµ‹åˆ°æ—§ç‰ˆ LayerNorm å‘½å (gamma/beta)")
        else:
            print(f"   ğŸ“Œ æ£€æµ‹åˆ°æ–°ç‰ˆ LayerNorm å‘½å (weight/bias)")

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return 1

    # è½¬æ¢æƒé‡
    try:
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®ï¼›å¦åˆ™è‡ªåŠ¨æ£€æµ‹
        if any([args.vocab_size, args.hidden, args.layers, args.heads]):
            # å…ˆè‡ªåŠ¨æ£€æµ‹ï¼Œç„¶åç”¨ç”¨æˆ·å‚æ•°è¦†ç›–
            model_config = detect_model_config(hf_state_dict)
            if args.vocab_size:
                model_config['vocab_size'] = args.vocab_size
            if args.hidden:
                model_config['hidden'] = args.hidden
            if args.layers:
                model_config['n_layers'] = args.layers
            if args.heads:
                model_config['attn_heads'] = args.heads

            print(f"\nğŸ“Š ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹é…ç½®:")
            print(f"  è¯æ±‡è¡¨å¤§å°:   {model_config['vocab_size']}")
            print(f"  éšè—å±‚ç»´åº¦:   {model_config['hidden']}")
            print(f"  Transformerå±‚: {model_config['n_layers']}")
            print(f"  æ³¨æ„åŠ›å¤´æ•°:   {model_config['attn_heads']}")

            converted_state_dict, model_config = convert_huggingface_to_bert_pytorch(hf_state_dict, model_config)
        else:
            # å®Œå…¨è‡ªåŠ¨æ£€æµ‹
            converted_state_dict, model_config = convert_huggingface_to_bert_pytorch(hf_state_dict)
    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    # ä¿å­˜è½¬æ¢åçš„æƒé‡
    print(f"\nğŸ’¾ ä¿å­˜è½¬æ¢åçš„æƒé‡åˆ°: {output_path}")
    try:
        torch.save(converted_state_dict, output_path)
        print(f"âœ… ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return 1

    # éªŒè¯
    print(f"\n" + "=" * 80)
    print("éªŒè¯è½¬æ¢ç»“æœ")
    print("=" * 80)

    try:
        # åˆ›å»º BERT æ¨¡å‹è¿›è¡ŒéªŒè¯
        sys.path.insert(0, '.')
        from bert_pytorch import BERT

        print(f"\nä½¿ç”¨é…ç½®åˆ›å»ºéªŒè¯æ¨¡å‹:")
        print(f"  vocab_size={model_config['vocab_size']}, hidden={model_config['hidden']}, "
              f"n_layers={model_config['n_layers']}, attn_heads={model_config['attn_heads']}")

        model = BERT(
            vocab_size=model_config['vocab_size'],
            hidden=model_config['hidden'],
            n_layers=model_config['n_layers'],
            attn_heads=model_config['attn_heads'],
            dropout=0.1
        )
        model_dict = model.state_dict()

        loaded = torch.load(output_path, map_location='cpu', weights_only=False)

        print(f"âœ… è½¬æ¢åçš„æƒé‡: {len(loaded)} ä¸ªå‚æ•°")
        print(f"âœ… æ¨¡å‹æœŸæœ›: {len(model_dict)} ä¸ªå‚æ•°")

        # æ£€æŸ¥åŒ¹é…åº¦
        matched = set(loaded.keys()) & set(model_dict.keys())
        print(f"âœ… åŒ¹é…çš„å‚æ•°: {len(matched)} ä¸ª")

        if len(matched) != len(loaded):
            print(f"âš ï¸  è­¦å‘Š: {len(loaded) - len(matched)} ä¸ªå‚æ•°æœªåŒ¹é…")

        # å°è¯•åŠ è½½
        result = model.load_state_dict(loaded, strict=False)

        if result.missing_keys:
            print(f"\nâš ï¸  æ¨¡å‹ä¸­ç¼ºå¤±çš„å‚æ•°: {len(result.missing_keys)} ä¸ª")
            for key in result.missing_keys[:5]:
                print(f"     - {key}")
            if len(result.missing_keys) > 5:
                print(f"     ... è¿˜æœ‰ {len(result.missing_keys) - 5} ä¸ª")

        if result.unexpected_keys:
            print(f"\nâš ï¸  è½¬æ¢æ–‡ä»¶ä¸­å¤šä½™çš„å‚æ•°: {len(result.unexpected_keys)} ä¸ª")
            for key in result.unexpected_keys[:5]:
                print(f"     - {key}")

        if not result.missing_keys and not result.unexpected_keys:
            print(f"\nğŸ‰ å®Œç¾ï¼æ‰€æœ‰å‚æ•°éƒ½æ­£ç¡®åŒ¹é…ï¼")

    except Exception as e:
        print(f"âš ï¸  éªŒè¯è·³è¿‡ï¼ˆæ— æ³•å¯¼å…¥ BERT æ¨¡å‹ï¼‰: {e}")

    print(f"\n" + "=" * 80)
    print("âœ… è½¬æ¢å®Œæˆï¼")
    print("=" * 80)
    print(f"\nç°åœ¨å¯ä»¥è¿è¡Œæ¨ç†è„šæœ¬:")
    print(f"  python3 bert_real_inference.py")

    return 0


if __name__ == "__main__":
    sys.exit(main())
