#!/usr/bin/env python3
"""
BERT Weight Converter
Converts PyTorch BERT model weights to .wts format for TensorRT
"""

import sys
import argparse
import os
import struct
import torch
import torch.nn as nn

# Import BERT model from bert_standalone.py
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from bert_standalone import BERT

def detect_model_config_from_weights(weights_file):
    """
    ä»Žæƒé‡æ–‡ä»¶ä¸­æ£€æµ‹æ¨¡åž‹é…ç½®

    Returns:
        dict: åŒ…å« vocab_size, hidden, n_layers, attn_heads
    """
    print(f"ðŸ” æ£€æµ‹æƒé‡æ–‡ä»¶çš„æ¨¡åž‹é…ç½®...")

    checkpoint = torch.load(weights_file, map_location='cpu', weights_only=False)

    # æ£€æµ‹ vocab_size
    if 'embedding.token.weight' in checkpoint:
        vocab_size = checkpoint['embedding.token.weight'].shape[0]
        hidden = checkpoint['embedding.token.weight'].shape[1]
    elif 'bert.embeddings.word_embeddings.weight' in checkpoint:
        vocab_size = checkpoint['bert.embeddings.word_embeddings.weight'].shape[0]
        hidden = checkpoint['bert.embeddings.word_embeddings.weight'].shape[1]
    else:
        raise ValueError("æ— æ³•æ£€æµ‹æ¨¡åž‹é…ç½®: æ‰¾ä¸åˆ° embedding å±‚")

    # æ£€æµ‹å±‚æ•°
    n_layers = 0
    for key in checkpoint.keys():
        if 'transformer_blocks.' in key:
            layer_num = int(key.split('.')[1])
            n_layers = max(n_layers, layer_num + 1)
        elif 'bert.encoder.layer.' in key:
            layer_num = int(key.split('.')[3])
            n_layers = max(n_layers, layer_num + 1)

    # æ£€æµ‹æ³¨æ„åŠ›å¤´æ•° (é€šå¸¸ head_dim = 64)
    attn_heads = hidden // 64

    model_type = "BERT-Large" if n_layers == 24 else "BERT-Base"

    config = {
        'vocab_size': vocab_size,
        'hidden': hidden,
        'n_layers': n_layers,
        'attn_heads': attn_heads,
        'model_type': model_type
    }

    print(f"  æ£€æµ‹åˆ°: {model_type}")
    print(f"  - è¯æ±‡è¡¨: {vocab_size}")
    print(f"  - éšè—å±‚: {hidden}")
    print(f"  - å±‚æ•°: {n_layers}")
    print(f"  - æ³¨æ„åŠ›å¤´: {attn_heads}")

    return config

def parse_args():
    parser = argparse.ArgumentParser(
        description='Convert BERT .bin file to .wts (æ”¯æŒ BERT-Base å’Œ BERT-Large)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è‡ªåŠ¨æ£€æµ‹æ¨¡åž‹é…ç½®
  python3 gen_bert_wts.py -w bert-base-uncased-converted.bin
  python3 gen_bert_wts.py -w bert-large-uncased-converted.bin -o bert-large.wts

  # ä½¿ç”¨é¢„è®¾é…ç½®
  python3 gen_bert_wts.py -w weights.bin --model-type bert-large

  # æ‰‹åŠ¨æŒ‡å®šé…ç½®
  python3 gen_bert_wts.py -w weights.bin --hidden 1024 --layers 24 --heads 16
        """
    )
    parser.add_argument('-w', '--weights', required=True,
                        help='Input weights (.bin) file path (required)')
    parser.add_argument('-o', '--output', help='Output (.wts) file path (optional)')
    parser.add_argument('--model-type', choices=['bert-base', 'bert-large', 'auto'],
                        default='auto',
                        help='Model type preset (default: auto-detect)')
    parser.add_argument('-v', '--vocab_size', type=int,
                        help='Vocabulary size (default: auto-detect)')
    parser.add_argument('--hidden', type=int,
                        help='Hidden dimension (default: auto-detect)')
    parser.add_argument('--layers', type=int,
                        help='Number of transformer layers (default: auto-detect)')
    parser.add_argument('--heads', type=int,
                        help='Number of attention heads (default: auto-detect)')
    args = parser.parse_args()
    
    if not os.path.isfile(args.weights):
        raise SystemExit('Invalid input file')
    
    if not args.output:
        args.output = os.path.splitext(args.weights)[0] + '.wts'
    elif os.path.isdir(args.output):
        args.output = os.path.join(
            args.output,
            os.path.splitext(os.path.basename(args.weights))[0] + '.wts')
    
    return args

def main():
    args = parse_args()

    print(f'=' * 70)
    print(f'BERT Weight Converter (.bin -> .wts)')
    print(f'=' * 70)
    print(f'Loading {args.weights}')

    # ç¡®å®šæ¨¡åž‹é…ç½®
    if args.model_type == 'bert-base':
        config = {
            'vocab_size': 30522,
            'hidden': 768,
            'n_layers': 12,
            'attn_heads': 12,
            'model_type': 'BERT-Base'
        }
        print(f'\nä½¿ç”¨ BERT-Base é¢„è®¾é…ç½®')
    elif args.model_type == 'bert-large':
        config = {
            'vocab_size': 30522,
            'hidden': 1024,
            'n_layers': 24,
            'attn_heads': 16,
            'model_type': 'BERT-Large'
        }
        print(f'\nä½¿ç”¨ BERT-Large é¢„è®¾é…ç½®')
    else:
        # è‡ªåŠ¨æ£€æµ‹
        config = detect_model_config_from_weights(args.weights)

    # ç”¨æˆ·æŒ‡å®šçš„å‚æ•°è¦†ç›–
    if args.vocab_size:
        config['vocab_size'] = args.vocab_size
    if args.hidden:
        config['hidden'] = args.hidden
    if args.layers:
        config['n_layers'] = args.layers
    if args.heads:
        config['attn_heads'] = args.heads

    print(f'\nðŸ“Š æœ€ç»ˆæ¨¡åž‹é…ç½®:')
    print(f"  æ¨¡åž‹ç±»åž‹: {config.get('model_type', 'Custom')}")
    print(f"  è¯æ±‡è¡¨:   {config['vocab_size']}")
    print(f"  éšè—å±‚:   {config['hidden']}")
    print(f"  å±‚æ•°:     {config['n_layers']}")
    print(f"  æ³¨æ„åŠ›å¤´: {config['attn_heads']}")

    # Create BERT model
    model = BERT(
        vocab_size=config['vocab_size'],
        hidden=config['hidden'],
        n_layers=config['n_layers'],
        attn_heads=config['attn_heads'],
        dropout=0.1
    )
    
    # Load weights
    device = 'cpu'
    checkpoint = torch.load(args.weights, map_location=device, weights_only=False)
    
    # Check if it's already in our format or HuggingFace format
    sample_key = list(checkpoint.keys())[0]
    if sample_key.startswith('embedding.') or sample_key.startswith('transformer_blocks.'):
        print('Detected converted format, loading directly...')
        model.load_state_dict(checkpoint, strict=False)
    else:
        print('Detected HuggingFace format, converting...')
        from bert_standalone import load_huggingface_weights
        # Save current state dict
        temp_file = '/tmp/bert_temp.bin'
        torch.save(checkpoint, temp_file)
        load_huggingface_weights(model, temp_file)
        os.remove(temp_file)
    
    model.to(device).eval()
    
    print(f'Model loaded successfully')
    print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')
    
    # Convert to .wts format
    print(f'Converting to .wts format...')
    state_dict = model.state_dict()
    
    with open(args.output, 'w') as f:
        f.write('{}\n'.format(len(state_dict.keys())))
        for k, v in state_dict.items():
            # ä¿æŒåŽŸå§‹å¼ é‡å½¢çŠ¶ï¼ŒæŒ‰è¡Œä¼˜å…ˆé¡ºåºå±•å¹³
            vr = v.flatten().cpu().numpy()
            f.write('{} {} '.format(k, len(vr)))
            for vv in vr:
                f.write(' ')
                f.write(struct.pack('>f', float(vv)).hex())
            f.write('\n')
    
    print(f'âœ… .wts file generated: {args.output}')
    print(f'Total tensors: {len(state_dict.keys())}')
    
    # Print some key tensor shapes for verification
    print('\nKey tensor shapes:')
    print(f'  Token embedding: {state_dict["embedding.token.weight"].shape}')
    print(f'  Position embedding: {state_dict["embedding.position.pe.weight"].shape}')
    print(f'  Segment embedding: {state_dict["embedding.segment.weight"].shape}')
    print(f'  First layer attention QKV: {state_dict["transformer_blocks.0.attention.linear_layers.0.weight"].shape}')

if __name__ == '__main__':
    main()

