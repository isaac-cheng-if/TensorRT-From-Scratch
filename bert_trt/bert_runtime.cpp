/*
 * BERT Runtime Inference
 * Handles engine deserialization, inference execution, and sentence similarity analysis
 */

#include <fstream>
#include <iostream>
#include <iomanip>
#include <map>
#include <vector>
#include <memory>
#include <chrono>
#include <cmath>
#include <cassert>
#include <sstream>
#include <numeric>
#include <algorithm>
#include <string>
#include <cctype>
#include <cstring>
#include <limits>

#include "NvInfer.h"
#include "cuda_runtime_api.h"

// ==================== Configuration ====================
struct BERTConfig {
    int BATCH_SIZE = 16;
    int MAX_SEQ_LEN = 128;
    int HIDDEN_DIM = 768;  // Will be auto-detected from engine
    const char* INPUT_IDS_NAME = "input_ids";
    const char* ATTENTION_MASK_NAME = "attention_mask";
    const char* SEGMENT_IDS_NAME = "token_type_ids";
    const char* OUTPUT_NAME = "output";
    int GPU_ID = 0;

    void DetectFromEngine(nvinfer1::ICudaEngine* engine) {
        // Auto-detect HIDDEN_DIM from engine output dimensions
        for (int i = 0; i < engine->getNbBindings(); i++) {
            if (!engine->bindingIsInput(i)) {
                // Found an output binding
                nvinfer1::Dims dims = engine->getBindingDimensions(i);
                // Output format: [batch, seq_len, hidden_dim]
                if (dims.nbDims == 3 && dims.d[2] > 0) {
                    HIDDEN_DIM = dims.d[2];
                    std::cout << "ğŸ” Auto-detected HIDDEN_DIM = " << HIDDEN_DIM << " from engine" << std::endl;
                    std::string model_type = (HIDDEN_DIM == 1024) ? "BERT-Large" : "BERT-Base";
                    std::cout << "   Model type: " << model_type << std::endl;
                    break;
                }
            }
        }
    }

    void Print() const {
        std::cout << "Configuration:" << std::endl;
        std::cout << "  â€¢ Batch size: " << BATCH_SIZE << std::endl;
        std::cout << "  â€¢ Max sequence length: " << MAX_SEQ_LEN << std::endl;
        std::cout << "  â€¢ Hidden dimension: " << HIDDEN_DIM << std::endl;
    }
};

BERTConfig Config;

// ==================== CUDA Error Checking ====================
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error: " << cudaGetErrorString(err) \
                      << " at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(1); \
        } \
    } while(0)


// ==================== TensorRT Logger ====================
class Logger : public nvinfer1::ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        if (severity <= Severity::kWARNING) {
            std::cout << "[TensorRT] " << msg << std::endl;
        }
    }
} gLogger;

// ==================== Performance Statistics ====================
struct PerfStats {
    int iterations;
    double avg_time;
    double min_time;
    double max_time;
    double total_time;
    double throughput;

    PerfStats() : iterations(0), avg_time(0), min_time(0), max_time(0),
                  total_time(0), throughput(0) {}

    void print(const std::string& title) const {
        std::cout << "\n=== " << title << " ===\n";
        std::cout << "Iterations: " << iterations << "\n";
        std::cout << "Average time: " << std::fixed << std::setprecision(3) << avg_time << " ms\n";
        std::cout << "Min time: " << std::fixed << std::setprecision(3) << min_time << " ms\n";
        std::cout << "Max time: " << std::fixed << std::setprecision(3) << max_time << " ms\n";
        std::cout << "Total time: " << std::fixed << std::setprecision(3) << total_time << " ms\n";
        std::cout << "Throughput: " << std::fixed << std::setprecision(2) << throughput << " sentences/sec\n";
        double batches_per_sec = throughput / Config.BATCH_SIZE;
        std::cout << "Batch Throughput: " << std::fixed << std::setprecision(2)
                  << batches_per_sec << " batches/sec (batch size: " << Config.BATCH_SIZE << ")\n";

        double tokens_per_batch = static_cast<double>(Config.BATCH_SIZE) * Config.MAX_SEQ_LEN;
        double tokens_per_sec = tokens_per_batch * batches_per_sec;
        std::cout << "Token Throughput: " << std::fixed << std::setprecision(2)
                << tokens_per_sec << " tokens/sec (" << tokens_per_batch << " tokens/batch)\n";

        std::cout << "====================================\n";
    }
};

// ==================== Binary Data Loader ====================
struct PreprocessedData {
    std::vector<std::vector<int>> input_ids_batch;
    std::vector<std::vector<int>> segment_ids_batch;
    std::vector<std::vector<int>> attention_masks_batch;
    std::vector<std::string> sentences;
    int batch_size;
    int max_seq_len;
};

bool LoadBinaryData(const std::string& data_prefix, PreprocessedData& data) {
    std::cout << "ğŸ“¥ åŠ è½½é¢„å¤„ç†æ•°æ®: " << data_prefix << std::endl;

    // æ–‡ä»¶è·¯å¾„
    std::string input_ids_file = data_prefix + "_input_ids.bin";
    std::string segment_ids_file = data_prefix + "_segment_ids.bin";
    std::string attention_masks_file = data_prefix + "_attention_masks.bin";
    std::string metadata_file = data_prefix + "_metadata.txt";

    // è¯»å–å…ƒæ•°æ®è·å–å½¢çŠ¶ä¿¡æ¯
    std::ifstream meta(metadata_file);
    if (!meta.is_open()) {
        std::cerr << "âŒ æ— æ³•æ‰“å¼€å…ƒæ•°æ®æ–‡ä»¶: " << metadata_file << std::endl;
        return false;
    }

    std::string line;
    int num_sentences = 0;
    data.batch_size = Config.BATCH_SIZE;
    data.max_seq_len = Config.MAX_SEQ_LEN;

    // è§£æå…ƒæ•°æ®
    bool in_sentences = false;
    while (std::getline(meta, line)) {
        if (line.find("å¥å­æ•°é‡:") != std::string::npos) {
            size_t pos = line.find(":");
            if (pos != std::string::npos) {
                num_sentences = std::stoi(line.substr(pos + 1));
            }
        }
        if (line.find("å¥å­åˆ—è¡¨:") != std::string::npos) {
            in_sentences = true;
            std::getline(meta, line); // Skip separator
            continue;
        }
        if (in_sentences && !line.empty() && line[0] == '[') {
            // æå–å¥å­
            size_t start = line.find("]") + 2;
            if (start < line.length()) {
                data.sentences.push_back(line.substr(start));
            }
        }
    }
    meta.close();

    std::cout << "  æ‰¹æ¬¡å¤§å°: " << data.batch_size << std::endl;
    std::cout << "  åºåˆ—é•¿åº¦: " << data.max_seq_len << std::endl;
    std::cout << "  å¥å­æ•°é‡: " << data.sentences.size() << std::endl;

    // è®¡ç®—æ€»å…ƒç´ æ•°
    size_t total_elements = data.batch_size * data.max_seq_len;
    size_t total_bytes = total_elements * sizeof(int32_t);

    // è¯»å– input_ids
    std::ifstream input_ids_f(input_ids_file, std::ios::binary);
    if (!input_ids_f.is_open()) {
        std::cerr << "âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶: " << input_ids_file << std::endl;
        return false;
    }

    std::vector<int32_t> input_ids_flat(total_elements);
    input_ids_f.read(reinterpret_cast<char*>(input_ids_flat.data()), total_bytes);
    input_ids_f.close();

    // è¯»å– segment_ids
    std::ifstream segment_ids_f(segment_ids_file, std::ios::binary);
    if (!segment_ids_f.is_open()) {
        std::cerr << "âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶: " << segment_ids_file << std::endl;
        return false;
    }

    std::vector<int32_t> segment_ids_flat(total_elements);
    segment_ids_f.read(reinterpret_cast<char*>(segment_ids_flat.data()), total_bytes);
    segment_ids_f.close();

    // è¯»å– attention_masks
    std::ifstream attention_masks_f(attention_masks_file, std::ios::binary);
    if (!attention_masks_f.is_open()) {
        std::cerr << "âŒ æ— æ³•æ‰“å¼€æ–‡ä»¶: " << attention_masks_file << std::endl;
        return false;
    }

    std::vector<int32_t> attention_masks_flat(total_elements);
    attention_masks_f.read(reinterpret_cast<char*>(attention_masks_flat.data()), total_bytes);
    attention_masks_f.close();

    // é‡å¡‘ä¸ºæ‰¹æ¬¡
    data.input_ids_batch.resize(data.batch_size);
    data.segment_ids_batch.resize(data.batch_size);
    data.attention_masks_batch.resize(data.batch_size);

    for (int b = 0; b < data.batch_size; b++) {
        data.input_ids_batch[b].resize(data.max_seq_len);
        data.segment_ids_batch[b].resize(data.max_seq_len);
        data.attention_masks_batch[b].resize(data.max_seq_len);

        for (int i = 0; i < data.max_seq_len; i++) {
            int idx = b * data.max_seq_len + i;
            data.input_ids_batch[b][i] = input_ids_flat[idx];
            data.segment_ids_batch[b][i] = segment_ids_flat[idx];
            data.attention_masks_batch[b][i] = attention_masks_flat[idx];
        }
    }

    std::cout << "âœ… æ•°æ®åŠ è½½å®Œæˆ" << std::endl;
    std::cout << "  Input IDs: " << total_bytes << " bytes" << std::endl;
    std::cout << "  Segment IDs: " << total_bytes << " bytes" << std::endl;
    std::cout << "  Attention Masks: " << total_bytes << " bytes" << std::endl;

    return true;
}

// ==================== BERT Runtime Class ====================
class BERTRuntime {
private:
    nvinfer1::IRuntime* runtime;
    nvinfer1::ICudaEngine* engine;
    nvinfer1::IExecutionContext* context;
    
    // ç»‘å®šç´¢å¼•ç¼“å­˜ï¼ˆé¿å…æ¯æ¬¡æ¨ç†éƒ½æŸ¥æ‰¾ï¼‰
    int input_ids_idx;
    int attention_mask_idx;
    int token_type_ids_idx;
    int output_idx;  // last_hidden_state
    int pooler_output_idx;  // pooler_output (ONNX ç¬¬äºŒä¸ªè¾“å‡ºï¼Œå¯é€‰)

    void* gpu_input_ids_buffer;
    void* gpu_attention_mask_buffer;
    void* gpu_segment_ids_buffer;
    void* gpu_output_buffer;  // last_hidden_state
    void* gpu_pooler_output_buffer;  // pooler_output (ONNX ç¬¬äºŒä¸ªè¾“å‡ºï¼Œå¯é€‰)
    void* cpu_output_buffer;
    
    int input_ids_size;
    int attention_mask_size;
    int segment_ids_size;
    int output_size;
    int pooler_output_size;

    cudaStream_t stream;

public:
    BERTRuntime() : runtime(nullptr), engine(nullptr), context(nullptr),
                    gpu_input_ids_buffer(nullptr), gpu_attention_mask_buffer(nullptr),
                    gpu_segment_ids_buffer(nullptr), gpu_output_buffer(nullptr),
                    gpu_pooler_output_buffer(nullptr), cpu_output_buffer(nullptr),
                    input_ids_size(0), attention_mask_size(0), segment_ids_size(0), 
                    output_size(0), pooler_output_size(0), stream(nullptr),
                    input_ids_idx(-1), attention_mask_idx(-1), 
                    token_type_ids_idx(-1), output_idx(-1), pooler_output_idx(-1) {}

    ~BERTRuntime() {
        if (stream) cudaStreamDestroy(stream);
        if (cpu_output_buffer) free(cpu_output_buffer);
        if (gpu_pooler_output_buffer) cudaFree(gpu_pooler_output_buffer);
        if (gpu_output_buffer) cudaFree(gpu_output_buffer);
        if (gpu_segment_ids_buffer) cudaFree(gpu_segment_ids_buffer);
        if (gpu_attention_mask_buffer) cudaFree(gpu_attention_mask_buffer);
        if (gpu_input_ids_buffer) cudaFree(gpu_input_ids_buffer);
        if (context) context->destroy();
        if (engine) engine->destroy();
        if (runtime) runtime->destroy();
    }

    bool Deserialize(const std::string& engine_file) {
        std::cout << "ğŸ“¥ åŠ è½½ TensorRT engine: " << engine_file << std::endl;

        std::ifstream file(engine_file, std::ios::binary);
        if (!file.good()) {
            std::cerr << "âŒ æ— æ³•è¯»å– engine æ–‡ä»¶: " << engine_file << std::endl;
            return false;
        }

        file.seekg(0, std::ios::end);
        size_t size = file.tellg();
        file.seekg(0, std::ios::beg);

        std::vector<char> engine_data(size);
        file.read(engine_data.data(), size);
        file.close();

        runtime = nvinfer1::createInferRuntime(gLogger);
        if (!runtime) {
            std::cerr << "âŒ createInferRuntime å¤±è´¥" << std::endl;
            return false;
        }

        engine = runtime->deserializeCudaEngine(engine_data.data(), size);
        if (!engine) {
            std::cerr << "âŒ deserializeCudaEngine å¤±è´¥" << std::endl;
            return false;
        }

        
        // Auto-detect configuration from engine
        Config.DetectFromEngine(engine);
        context = engine->createExecutionContext();
        if (!context) {
            std::cerr << "âŒ createExecutionContext å¤±è´¥" << std::endl;
            return false;
        }

        // æ£€æŸ¥å¼•æ“çš„è¾“å…¥è¾“å‡ºä¿¡æ¯ï¼ˆå…¼å®¹ ONNX å’Œè‡ªå®šä¹‰ builderï¼‰
        int num_inputs = 0;
        int num_outputs = 0;
        for (int i = 0; i < engine->getNbBindings(); i++) {
            if (engine->bindingIsInput(i)) {
                num_inputs++;
            } else {
                num_outputs++;
            }
        }

        std::cout << "ğŸ“Š Engine ä¿¡æ¯:" << std::endl;
        std::cout << "  è¾“å…¥æ•°é‡: " << num_inputs << std::endl;
        std::cout << "  è¾“å‡ºæ•°é‡: " << num_outputs << std::endl;
        
        // æ‰“å°æ‰€æœ‰ç»‘å®šä¿¡æ¯
        for (int i = 0; i < engine->getNbBindings(); i++) {
            const char* name = engine->getBindingName(i);
            bool is_input = engine->bindingIsInput(i);
            nvinfer1::Dims dims = engine->getBindingDimensions(i);
            std::cout << "  [" << i << "] " << (is_input ? "è¾“å…¥" : "è¾“å‡º") 
                      << ": " << (name ? name : "unknown") << " [";
            for (int j = 0; j < dims.nbDims; j++) {
                std::cout << dims.d[j];
                if (j < dims.nbDims - 1) std::cout << ", ";
            }
            std::cout << "]" << std::endl;
        }

        // Allocate GPU memory
        input_ids_size = Config.BATCH_SIZE * Config.MAX_SEQ_LEN * sizeof(int32_t);
        attention_mask_size = Config.BATCH_SIZE * Config.MAX_SEQ_LEN * sizeof(int32_t);
        segment_ids_size = Config.BATCH_SIZE * Config.MAX_SEQ_LEN * sizeof(int32_t);
        
        // ONNX è¾“å‡º: last_hidden_state [B, N, D] å’Œ pooler_output [B, D]
        // è‡ªå®šä¹‰ builder è¾“å‡º: output [B, N, D]
        // æˆ‘ä»¬åªéœ€è¦ last_hidden_state (ç¬¬ä¸€ä¸ªè¾“å‡º)ï¼Œä½†éœ€è¦ä¸º pooler_output ä¹Ÿåˆ†é…ç¼“å†²åŒº
        output_size = Config.BATCH_SIZE * Config.MAX_SEQ_LEN * Config.HIDDEN_DIM * sizeof(float);
        pooler_output_size = Config.BATCH_SIZE * Config.HIDDEN_DIM * sizeof(float);
        
        CUDA_CHECK(cudaMalloc(&gpu_input_ids_buffer, input_ids_size));
        CUDA_CHECK(cudaMalloc(&gpu_attention_mask_buffer, attention_mask_size));
        CUDA_CHECK(cudaMalloc(&gpu_segment_ids_buffer, segment_ids_size));
        CUDA_CHECK(cudaMalloc(&gpu_output_buffer, output_size));
        // ä¸º pooler_output åˆ†é…ç¼“å†²åŒºï¼ˆå³ä½¿æˆ‘ä»¬ä¸ä½¿ç”¨å®ƒï¼‰
        CUDA_CHECK(cudaMalloc(&gpu_pooler_output_buffer, pooler_output_size));
        cpu_output_buffer = malloc(output_size);
        
        CUDA_CHECK(cudaStreamCreate(&stream));
        
        // æŸ¥æ‰¾å¹¶ç¼“å­˜ç»‘å®šç´¢å¼•ï¼ˆå…¼å®¹ ONNX å’Œè‡ªå®šä¹‰ builderï¼‰
        input_ids_idx = -1;
        attention_mask_idx = -1;
        token_type_ids_idx = -1;
        output_idx = -1;
        pooler_output_idx = -1;
        
        for (int i = 0; i < engine->getNbBindings(); i++) {
            const char* name = engine->getBindingName(i);
            if (!name) continue;
            
            if (engine->bindingIsInput(i)) {
                if (strcmp(name, Config.INPUT_IDS_NAME) == 0) input_ids_idx = i;
                else if (strcmp(name, Config.ATTENTION_MASK_NAME) == 0) attention_mask_idx = i;
                else if (strcmp(name, Config.SEGMENT_IDS_NAME) == 0) token_type_ids_idx = i;
            } else {
                // è¾“å‡º: ä¼˜å…ˆä½¿ç”¨ last_hidden_state (ONNX)ï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡º
                if (strcmp(name, "last_hidden_state") == 0 || strcmp(name, Config.OUTPUT_NAME) == 0) {
                    if (output_idx == -1) output_idx = i;
                } else if (strcmp(name, "pooler_output") == 0) {
                    pooler_output_idx = i;
                } else if (output_idx == -1 && i >= 3) {
                    output_idx = i;
                }
            }
        }
        
        // å¦‚æœæ‰¾ä¸åˆ°åç§°åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤é¡ºåºï¼ˆå‡è®¾æ ‡å‡†é¡ºåºï¼‰
        if (input_ids_idx == -1) input_ids_idx = 0;
        if (attention_mask_idx == -1) attention_mask_idx = 1;
        if (token_type_ids_idx == -1) token_type_ids_idx = 2;
        if (output_idx == -1) {
            // æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¾“å‡ºç»‘å®š
            for (int i = 0; i < engine->getNbBindings(); i++) {
                if (!engine->bindingIsInput(i)) {
                    output_idx = i;
                    break;
                }
            }
        }
        // å¦‚æœæ‰¾ä¸åˆ° pooler_outputï¼Œè®¾ç½®ä¸º -1ï¼ˆè¡¨ç¤ºä¸å­˜åœ¨ï¼‰
        if (pooler_output_idx == -1) {
            // æŸ¥æ‰¾ç¬¬äºŒä¸ªè¾“å‡ºï¼ˆå¦‚æœæœ‰ï¼‰
            int output_count = 0;
            for (int i = 0; i < engine->getNbBindings(); i++) {
                if (!engine->bindingIsInput(i)) {
                    output_count++;
                    if (output_count == 2 && i != output_idx) {
                        pooler_output_idx = i;
                        break;
                    }
                }
            }
        }
        
        std::cout << "\nâœ… Engine åŠ è½½æˆåŠŸ!" << std::endl;
        std::cout << "  ç»‘å®šç´¢å¼•: input_ids=" << input_ids_idx 
                  << ", attention_mask=" << attention_mask_idx
                  << ", token_type_ids=" << token_type_ids_idx
                  << ", output=" << output_idx;
        if (pooler_output_idx != -1) {
            std::cout << ", pooler_output=" << pooler_output_idx;
        }
        std::cout << std::endl;
        std::cout << "  Input IDs å¤§å°: " << input_ids_size << " bytes" << std::endl;
        std::cout << "  Attention Mask å¤§å°: " << attention_mask_size << " bytes" << std::endl;
        std::cout << "  Token Type IDs å¤§å°: " << segment_ids_size << " bytes" << std::endl;
        std::cout << "  è¾“å‡ºå¤§å° (last_hidden_state): " << output_size << " bytes" << std::endl;
        if (pooler_output_idx != -1) {
            std::cout << "  è¾“å‡ºå¤§å° (pooler_output): " << pooler_output_size << " bytes" << std::endl;
        }

        return true;
    }

    bool InferBatch(const std::vector<std::vector<int>>& input_ids_batch,
                   const std::vector<std::vector<int>>& attention_mask_batch,
                   const std::vector<std::vector<int>>& segment_ids_batch,
                   std::vector<std::vector<std::vector<float>>>& outputs) {
        
        if (input_ids_batch.size() != Config.BATCH_SIZE || 
            attention_mask_batch.size() != Config.BATCH_SIZE ||
            segment_ids_batch.size() != Config.BATCH_SIZE) {
            std::cerr << "ERROR: Batch size must be " << Config.BATCH_SIZE << std::endl;
            return false;
        }
        
        // Prepare input data
        std::vector<int32_t> input_ids_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        std::vector<int32_t> attention_mask_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        std::vector<int32_t> segment_ids_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        
        for (int b = 0; b < Config.BATCH_SIZE; b++) {
            for (int i = 0; i < Config.MAX_SEQ_LEN; i++) {
                input_ids_data[b * Config.MAX_SEQ_LEN + i] = input_ids_batch[b][i];
                attention_mask_data[b * Config.MAX_SEQ_LEN + i] = attention_mask_batch[b][i];
                segment_ids_data[b * Config.MAX_SEQ_LEN + i] = segment_ids_batch[b][i];
            }
        }
        
        // Copy to GPU
        CUDA_CHECK(cudaMemcpyAsync(gpu_input_ids_buffer, input_ids_data.data(), input_ids_size, 
                                   cudaMemcpyHostToDevice, stream));
        CUDA_CHECK(cudaMemcpyAsync(gpu_attention_mask_buffer, attention_mask_data.data(), attention_mask_size, 
                                   cudaMemcpyHostToDevice, stream));
        CUDA_CHECK(cudaMemcpyAsync(gpu_segment_ids_buffer, segment_ids_data.data(), segment_ids_size, 
                                   cudaMemcpyHostToDevice, stream));
        
        // Run inference
        // executeV2 éœ€è¦æŒ‰ç…§å¼•æ“ç»‘å®šç´¢å¼•é¡ºåºä¼ é€’ç¼“å†²åŒº
        // ä½¿ç”¨ç¼“å­˜çš„ç»‘å®šç´¢å¼•ï¼ˆåœ¨ Deserialize ä¸­å·²è®¾ç½®ï¼‰
        
        // åˆ›å»º bindings æ•°ç»„ï¼ˆæŒ‰å¼•æ“ç»‘å®šç´¢å¼•é¡ºåºï¼‰
        // executeV2 è¦æ±‚æ‰€æœ‰ç»‘å®šéƒ½æœ‰æœ‰æ•ˆçš„ç¼“å†²åŒºæŒ‡é’ˆï¼ˆå³ä½¿æˆ‘ä»¬ä¸ä½¿ç”¨æŸäº›è¾“å‡ºï¼‰
        std::vector<void*> bindings(engine->getNbBindings(), nullptr);
        bindings[input_ids_idx] = gpu_input_ids_buffer;
        bindings[attention_mask_idx] = gpu_attention_mask_buffer;
        bindings[token_type_ids_idx] = gpu_segment_ids_buffer;
        bindings[output_idx] = gpu_output_buffer;
        if (pooler_output_idx != -1) {
            bindings[pooler_output_idx] = gpu_pooler_output_buffer;
        }
        
        bool status = context->executeV2(bindings.data());
        if (!status) {
            std::cerr << "ERROR: executeV2 failed" << std::endl;
            return false;
        }

        // Copy result back to CPU
        CUDA_CHECK(cudaMemcpyAsync(cpu_output_buffer, gpu_output_buffer, output_size,
                                   cudaMemcpyDeviceToHost, stream));
        CUDA_CHECK(cudaStreamSynchronize(stream));

        // Convert to vector format [batch][seq_len][hidden_dim]
        float* output_ptr = static_cast<float*>(cpu_output_buffer);
        outputs.resize(Config.BATCH_SIZE);
        for (int b = 0; b < Config.BATCH_SIZE; b++) {
            outputs[b].resize(Config.MAX_SEQ_LEN);
            for (int s = 0; s < Config.MAX_SEQ_LEN; s++) {
                outputs[b][s].resize(Config.HIDDEN_DIM);
                int offset = (b * Config.MAX_SEQ_LEN + s) * Config.HIDDEN_DIM;
                for (int h = 0; h < Config.HIDDEN_DIM; h++) {
                    outputs[b][s][h] = output_ptr[offset + h];
                }
            }
        }

        return true;
    }

    std::vector<float> GetCLSEmbedding(const std::vector<std::vector<float>>& sequence_output) {
        // Extract [CLS] token embedding (first token)
        return sequence_output[0];
    }

    std::vector<float> GetMeanPooling(const std::vector<std::vector<float>>& sequence_output,
                                     const std::vector<int>& input_ids) {
        // Mean pooling over non-padding tokens
        std::vector<float> mean_embedding(Config.HIDDEN_DIM, 0.0f);
        int valid_tokens = 0;

        for (int i = 0; i < Config.MAX_SEQ_LEN; i++) {
            if (input_ids[i] != 0) { // Not padding
                for (int h = 0; h < Config.HIDDEN_DIM; h++) {
                    mean_embedding[h] += sequence_output[i][h];
                }
                valid_tokens++;
            }
        }

        if (valid_tokens > 0) {
            for (int h = 0; h < Config.HIDDEN_DIM; h++) {
                mean_embedding[h] /= valid_tokens;
            }
        }

        return mean_embedding;
    }

    float CosineSimilarity(const std::vector<float>& v1, const std::vector<float>& v2) {
        float dot_product = 0.0f;
        float norm1 = 0.0f;
        float norm2 = 0.0f;

        for (size_t i = 0; i < v1.size(); i++) {
            dot_product += v1[i] * v2[i];
            norm1 += v1[i] * v1[i];
            norm2 += v2[i] * v2[i];
        }

        if (norm1 == 0.0f || norm2 == 0.0f) return 0.0f;

        return dot_product / (std::sqrt(norm1) * std::sqrt(norm2));
    }

    float EuclideanDistance(const std::vector<float>& v1, const std::vector<float>& v2) {
        float sum = 0.0f;
        for (size_t i = 0; i < v1.size(); i++) {
            float diff = v1[i] - v2[i];
            sum += diff * diff;
        }
        return std::sqrt(sum);
    }

    PerfStats PerformanceTest(const std::vector<std::vector<int>>& input_ids_batch,
                            const std::vector<std::vector<int>>& attention_mask_batch,
                            const std::vector<std::vector<int>>& segment_ids_batch,
                            int iterations = 100) {
        PerfStats stats;
        stats.iterations = iterations;
        
        if (input_ids_batch.size() != Config.BATCH_SIZE ||
            attention_mask_batch.size() != Config.BATCH_SIZE ||
            segment_ids_batch.size() != Config.BATCH_SIZE) {
            std::cerr << "ERROR: Batch size must be " << Config.BATCH_SIZE << std::endl;
            return stats;
        }
        
        std::vector<double> times;
        times.reserve(iterations);
        
        // Warmup
        std::vector<std::vector<std::vector<float>>> dummy_outputs;
        for (int i = 0; i < 10; i++) {
            InferBatch(input_ids_batch, attention_mask_batch, segment_ids_batch, dummy_outputs);
        }
        
        // Prepare input data once
        std::vector<int32_t> input_ids_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        std::vector<int32_t> attention_mask_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        std::vector<int32_t> segment_ids_data(Config.BATCH_SIZE * Config.MAX_SEQ_LEN);
        
        for (int b = 0; b < Config.BATCH_SIZE; b++) {
            for (int i = 0; i < Config.MAX_SEQ_LEN; i++) {
                input_ids_data[b * Config.MAX_SEQ_LEN + i] = input_ids_batch[b][i];
                attention_mask_data[b * Config.MAX_SEQ_LEN + i] = attention_mask_batch[b][i];
                segment_ids_data[b * Config.MAX_SEQ_LEN + i] = segment_ids_batch[b][i];
            }
        }
        
        // Performance test with CUDA events
        cudaEvent_t start_event, end_event;
        CUDA_CHECK(cudaEventCreate(&start_event));
        CUDA_CHECK(cudaEventCreate(&end_event));
        
        for (int i = 0; i < iterations; i++) {
            // Copy to GPU
            CUDA_CHECK(cudaMemcpyAsync(gpu_input_ids_buffer, input_ids_data.data(), input_ids_size,
                                       cudaMemcpyHostToDevice, stream));
            CUDA_CHECK(cudaMemcpyAsync(gpu_attention_mask_buffer, attention_mask_data.data(), attention_mask_size,
                                       cudaMemcpyHostToDevice, stream));
            CUDA_CHECK(cudaMemcpyAsync(gpu_segment_ids_buffer, segment_ids_data.data(), segment_ids_size,
                                       cudaMemcpyHostToDevice, stream));
            
            // Start timing
            CUDA_CHECK(cudaEventRecord(start_event, stream));
            
            // Run inference (ä½¿ç”¨ç¼“å­˜çš„ç»‘å®šç´¢å¼•)
            std::vector<void*> perf_bindings(engine->getNbBindings(), nullptr);
            perf_bindings[input_ids_idx] = gpu_input_ids_buffer;
            perf_bindings[attention_mask_idx] = gpu_attention_mask_buffer;
            perf_bindings[token_type_ids_idx] = gpu_segment_ids_buffer;
            perf_bindings[output_idx] = gpu_output_buffer;
            if (pooler_output_idx != -1) {
                perf_bindings[pooler_output_idx] = gpu_pooler_output_buffer;
            }
            bool status = context->executeV2(perf_bindings.data());
            if (!status) {
                std::cerr << "ERROR: executeV2 failed" << std::endl;
                return stats;
            }

            // End timing
            CUDA_CHECK(cudaEventRecord(end_event, stream));
            CUDA_CHECK(cudaEventSynchronize(end_event));

            float gpu_time_ms;
            CUDA_CHECK(cudaEventElapsedTime(&gpu_time_ms, start_event, end_event));
            times.push_back(gpu_time_ms);

            // Copy result back (not timed)
            CUDA_CHECK(cudaMemcpyAsync(cpu_output_buffer, gpu_output_buffer, output_size,
                                       cudaMemcpyDeviceToHost, stream));
            CUDA_CHECK(cudaStreamSynchronize(stream));
        }

        CUDA_CHECK(cudaEventDestroy(start_event));
        CUDA_CHECK(cudaEventDestroy(end_event));

        // Calculate statistics
        stats.total_time = std::accumulate(times.begin(), times.end(), 0.0);
        stats.avg_time = stats.total_time / iterations;
        stats.min_time = *std::min_element(times.begin(), times.end());
        stats.max_time = *std::max_element(times.begin(), times.end());
        stats.throughput = (Config.BATCH_SIZE * 1000.0) / stats.avg_time;

        return stats;
    }

};

// ==================== Main Function ====================
int main(int argc, char** argv) {
    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "BERT Runtime Inference (Batch " << Config.BATCH_SIZE << ")" << std::endl;
    std::cout << "TensorRT Engine Inference" << std::endl;
    std::cout << std::string(70, '=') << "\n" << std::endl;

    if (argc < 3) {
        std::cout << "ç”¨æ³•:" << std::endl;
        std::cout << "  ./bert_runtime <engine_file> <data_prefix>" << std::endl;
        std::cout << "\nå‚æ•°:" << std::endl;
        std::cout << "  engine_file  - TensorRT engine æ–‡ä»¶è·¯å¾„" << std::endl;
        std::cout << "  data_prefix  - é¢„å¤„ç†æ•°æ®æ–‡ä»¶å‰ç¼€" << std::endl;
        std::cout << "\nç¤ºä¾‹:" << std::endl;
        std::cout << "  ./bert_runtime bert_base_batch16.engine bert_input" << std::endl;
        std::cout << "\nå‡†å¤‡æ•°æ®:" << std::endl;
        std::cout << "  åœ¨æœ‰ Python ç¯å¢ƒçš„æœºå™¨ä¸Šè¿è¡Œ:" << std::endl;
        std::cout << "  python3 prepare_bert_input.py -i sentences.txt -o bert_input" << std::endl;
        std::cout << "\n  ç„¶åå°†ç”Ÿæˆçš„ .bin æ–‡ä»¶ä¼ è¾“åˆ° Orin è®¾å¤‡" << std::endl;
        return -1;
    }

    cudaSetDevice(Config.GPU_ID);

    // åŠ è½½é¢„å¤„ç†æ•°æ®
    std::cout << "æ­¥éª¤ 1: åŠ è½½é¢„å¤„ç†æ•°æ®" << std::endl;
    std::cout << std::string(70, '-') << std::endl;

    PreprocessedData data;
    if (!LoadBinaryData(argv[2], data)) {
        std::cerr << "âŒ åŠ è½½æ•°æ®å¤±è´¥" << std::endl;
        return -1;
    }

    std::cout << std::endl;

    // åŠ è½½ engine
    std::cout << "æ­¥éª¤ 2: åŠ è½½ TensorRT Engine" << std::endl;
    std::cout << std::string(70, '-') << std::endl;

    BERTRuntime runtime;
    if (!runtime.Deserialize(argv[1])) {
        std::cerr << "âŒ Engine åŠ è½½å¤±è´¥" << std::endl;
        return -1;
    }

    std::cout << std::endl;

    // è¿è¡Œæ¨ç†
    std::cout << "æ­¥éª¤ 3: è¿è¡Œæ¨ç†" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    std::vector<std::vector<std::vector<float>>> outputs;
    if (runtime.InferBatch(data.input_ids_batch, data.attention_masks_batch, data.segment_ids_batch, outputs)) {
        std::cout << "âœ… æ¨ç†æˆåŠŸ!\n" << std::endl;

        // æå– embeddings (ä½¿ç”¨ mean pooling)
        std::cout << "æ­¥éª¤ 4: æå–å¥å­åµŒå…¥" << std::endl;
        std::cout << std::string(70, '-') << std::endl;

        std::vector<std::vector<float>> embeddings;
        int num_valid = std::min(Config.BATCH_SIZE, (int)data.sentences.size());

        for (int b = 0; b < num_valid; b++) {
            auto embedding = runtime.GetMeanPooling(outputs[b], data.input_ids_batch[b]);
            embeddings.push_back(embedding);

            if (b < 3) {  // åªæ˜¾ç¤ºå‰3ä¸ª
                std::cout << "  [" << b << "] \"" << data.sentences[b].substr(0, 50) << "...\"" << std::endl;
                std::cout << "      åµŒå…¥ç»´åº¦: " << embedding.size()
                          << ", èŒƒå›´: [" << *std::min_element(embedding.begin(), embedding.end())
                          << ", " << *std::max_element(embedding.begin(), embedding.end()) << "]" << std::endl;
            }
        }

        std::cout << "âœ… å…±æå– " << embeddings.size() << " ä¸ªå¥å­åµŒå…¥\n" << std::endl;

        // è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        std::cout << "æ­¥éª¤ 5: è®¡ç®—ç›¸ä¼¼åº¦" << std::endl;
        std::cout << std::string(70, '=') << std::endl;

        std::cout << "\nå¥å­åˆ—è¡¨:" << std::endl;
        for (int i = 0; i < num_valid; i++) {
            std::cout << "  [" << i << "] \"" << data.sentences[i] << "\"" << std::endl;
        }

        std::cout << "\nä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ:" << std::endl;
        std::cout << "      ";
        for (int i = 0; i < num_valid; i++) {
            std::cout << "   [" << i << "]  ";
        }
        std::cout << std::endl;

        for (int i = 0; i < num_valid; i++) {
            std::cout << " [" << i << "]  ";
            for (int j = 0; j < num_valid; j++) {
                float cos_sim = runtime.CosineSimilarity(embeddings[i], embeddings[j]);
                std::cout << std::fixed << std::setprecision(4) << cos_sim << "  ";
            }
            std::cout << std::endl;
        }

        // è®¡ç®—æ‰€æœ‰å¥å­å¯¹çš„ç›¸ä¼¼åº¦
        std::vector<std::tuple<int, int, float, float>> similarities;
        for (int i = 0; i < num_valid; i++) {
            for (int j = i + 1; j < num_valid; j++) {
                float cos_sim = runtime.CosineSimilarity(embeddings[i], embeddings[j]);
                float euc_dist = runtime.EuclideanDistance(embeddings[i], embeddings[j]);
                similarities.push_back({i, j, cos_sim, euc_dist});
            }
        }

        // æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        std::sort(similarities.begin(), similarities.end(),
                 [](const auto& a, const auto& b) { return std::get<2>(a) > std::get<2>(b); });

        // æœ€ç›¸ä¼¼çš„å¥å­å¯¹
        std::cout << "\nğŸ¯ æœ€ç›¸ä¼¼çš„å¥å­å¯¹:" << std::endl;
        std::cout << std::string(70, '-') << std::endl;

        for (int k = 0; k < std::min(5, (int)similarities.size()); k++) {
            int i = std::get<0>(similarities[k]);
            int j = std::get<1>(similarities[k]);
            float cos_sim = std::get<2>(similarities[k]);
            float euc_dist = std::get<3>(similarities[k]);

            std::cout << "\n" << (k+1) << ". å¥å­ " << i << " <-> å¥å­ " << j << std::endl;
            std::cout << "   A: \"" << data.sentences[i] << "\"" << std::endl;
            std::cout << "   B: \"" << data.sentences[j] << "\"" << std::endl;
            std::cout << "   ä½™å¼¦ç›¸ä¼¼åº¦: " << std::fixed << std::setprecision(6) << cos_sim << std::endl;
            std::cout << "   æ¬§å¼è·ç¦»:   " << std::fixed << std::setprecision(4) << euc_dist << std::endl;
        }

        std::cout << "\n" << std::string(70, '-') << std::endl;

        // æœ€ä¸ç›¸ä¼¼çš„å¥å­å¯¹
        std::cout << "\nâŒ æœ€ä¸ç›¸ä¼¼çš„å¥å­å¯¹:" << std::endl;
        std::cout << std::string(70, '-') << std::endl;

        int start_idx = std::max(0, (int)similarities.size() - 5);
        for (int k = similarities.size() - 1; k >= start_idx && k >= 0; k--) {
            int i = std::get<0>(similarities[k]);
            int j = std::get<1>(similarities[k]);
            float cos_sim = std::get<2>(similarities[k]);
            float euc_dist = std::get<3>(similarities[k]);

            std::cout << "\n" << (similarities.size() - k) << ". å¥å­ " << i << " <-> å¥å­ " << j << std::endl;
            std::cout << "   A: \"" << data.sentences[i] << "\"" << std::endl;
            std::cout << "   B: \"" << data.sentences[j] << "\"" << std::endl;
            std::cout << "   ä½™å¼¦ç›¸ä¼¼åº¦: " << std::fixed << std::setprecision(6) << cos_sim << std::endl;
            std::cout << "   æ¬§å¼è·ç¦»:   " << std::fixed << std::setprecision(4) << euc_dist << std::endl;
        }

        // ç»Ÿè®¡ä¿¡æ¯
        std::cout << "\n" << std::string(70, '=') << std::endl;
        std::cout << "\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:" << std::endl;

        float min_cos = std::get<2>(similarities.back());
        float max_cos = std::get<2>(similarities.front());
        float sum_cos = 0.0f;
        float min_euc = std::numeric_limits<float>::max();
        float max_euc = 0.0f;
        float sum_euc = 0.0f;

        for (const auto& sim : similarities) {
            sum_cos += std::get<2>(sim);
            float euc = std::get<3>(sim);
            sum_euc += euc;
            min_euc = std::min(min_euc, euc);
            max_euc = std::max(max_euc, euc);
        }

        float avg_cos = sum_cos / similarities.size();
        float avg_euc = sum_euc / similarities.size();

        std::cout << "  å¥å­å¯¹æ€»æ•°:           " << similarities.size() << std::endl;
        std::cout << "  ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´:       [" << std::fixed << std::setprecision(4)
                  << min_cos << ", " << max_cos << "]" << std::endl;
        std::cout << "  ä½™å¼¦ç›¸ä¼¼åº¦å¹³å‡å€¼:     " << std::fixed << std::setprecision(4) << avg_cos << std::endl;
        std::cout << "  æ¬§å¼è·ç¦»èŒƒå›´:         [" << std::fixed << std::setprecision(4)
                  << min_euc << ", " << max_euc << "]" << std::endl;
        std::cout << "  æ¬§å¼è·ç¦»å¹³å‡å€¼:       " << std::fixed << std::setprecision(4) << avg_euc << std::endl;

        std::cout << "\nğŸ’¡ ç›¸ä¼¼åº¦è§£é‡Š:" << std::endl;
        std::cout << "  â€¢ ä½™å¼¦ç›¸ä¼¼åº¦: 1.0 = å®Œå…¨ç›¸åŒ, 0.0 = å®Œå…¨ä¸ç›¸å…³, æ¥è¿‘1è¡¨ç¤ºç›¸ä¼¼" << std::endl;
        std::cout << "  â€¢ æ¬§å¼è·ç¦»:   < 6.0 = ç›¸ä¼¼, > 8.0 = ä¸ç›¸å…³, è¶Šå°è¶Šç›¸ä¼¼" << std::endl;

        // æ€§èƒ½æµ‹è¯•
        std::cout << "\n" << std::string(70, '=') << std::endl;
        std::cout << "æ­¥éª¤ 6: æ€§èƒ½åŸºå‡†æµ‹è¯•" << std::endl;
        std::cout << std::string(70, '=') << std::endl;

        auto perf_stats = runtime.PerformanceTest(data.input_ids_batch, data.attention_masks_batch, data.segment_ids_batch, 100);
        perf_stats.print("GPU æ€§èƒ½ (æ‰¹æ¬¡å¤§å° " + std::to_string(Config.BATCH_SIZE) + ")");
    } else {
        std::cerr << "âŒ æ¨ç†å¤±è´¥" << std::endl;
        return -1;
    }

    std::cout << "\n" << std::string(70, '=') << std::endl;
    std::cout << "âœ… æ¨ç†å®Œæˆ!" << std::endl;
    std::cout << std::string(70, '=') << std::endl;

    return 0;
}